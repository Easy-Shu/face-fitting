Tutorial / Bin Scripts
======================

There are some scripts in the ``bin`` directory that show some of the applications. There are several comments in these scripts to review, but for now, we will give an overview of these scripts.

Processing a 3DMM (``processBFM2017.py``)
-----------------------------------------

This module contains a function that we use to process the `Basel Face Model 2017 <https://faces.dmi.unibas.ch/bfm/bfm2017.html>`_ 3DMM. Such a model is manipulated through the :class:`MeshModel` class object, which stores the eigenmodel of the 3DMM.

This eigenmodel consists of the following models.

* Shape model: controls the location of the vertex coordinates, and is composed of two submodels:

	* Shape identity: controls the appearance of the neutral face
	* Shape facial expressions: manipulations of the neutral face to make certain facial expressions

* Texture model: controls the RGB color values of the vertices

There is a similar module, ``processFacewarehouse.py``, that contains functions that can be used to process the `FaceWarehouse <http://gaps-zju.org/facewarehouse/>`_ dataset to form a 3DMM.

Fitting a 3DMM to a video using depth maps (``vol2mesh.py``)
--------------------------------------------------------------

The goal of this script is to fit a 3DMM to each frame of a video by fitting the 3DMM to a depth map of the speaker in the video. The depth map is derived from a volume that is generated by the `VRN <https://github.com/AaronJackson/vrn>`_ for each frame in the video.

Because this script uses many external files generated from other sources, we organize a directory for a given video in the following manner. ::

	speaker-name/
	|--orig/
	|--scaled/
	|--volume/
	|--landmark/
	`--crop.tmp

* The root directory ``speaker-name/`` is perhaps the name of the speaker in the video.

	* The sub-directory ``orig/`` contains the frames/images from the source video.
	* ``scaled/`` contains the cropped and scaled version of the video frames as produced by the VRN.
	* ``volume/`` contains the ``.raw`` files produced by the VRN.
	* ``landmark/`` contains the `OpenPose <https://github.com/CMU-Perceptual-Computing-Lab/openpose>`_ landmarks on the original video frames
	* The file ``crop.tmp`` is produced by the VRN, but it deleted in one of the lines in the VRN shell script. Therefore, you need to comment out or remove that line to retain this file, which is used to find the transformation between the original frames and the cropped and scaled images.

Note that if the source video only features one speaker, then the shape identity parameters do not have to be optimized for each frame in the video. This is the case in this script, so we only optimize over the shape identity parameters for the first 20 frames in the video.

Use 3DMMs to reenact speech from an audio file (``speech2frame.py``)
--------------------------------------------------------------------

In this script, we use the 3DMM fittings for the video frames, as generated by ``vol2mesh.py``, in addition to the source audio file of the video to generate a temporally-coherent sequence of 3DMMs that appears to speak the words in a target audio file.

Because the source and target audio files may have different volumes, we normalize the audio signals to the same maximum amplitude by using a tool such as `Audacity <https://www.audacityteam.org/>`_.

To ensure temporal consistency in the reenactment sequence of 3DMMs, we find the shortest path through a series of candidate 3DMMs as measured by a distance metric that incorporates the 3DMM shape facial expression parameters, certain functions of the landmark coordinates, and the similarity transform parameters.

Using OpenGL to render the 3DMMs (``renderTest.py``)
----------------------------------------------------

Fitting a 3DMM to an RGB image (``img2mesh.py``)
------------------------------------------------
